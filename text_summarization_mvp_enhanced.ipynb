{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZL1OqQ8Em3r"
      },
      "source": [
        "# üöÄ Text Summarization MVP with Export Features\n",
        "\n",
        "## Enhanced Version with Multiple Export Formats\n",
        "\n",
        "This notebook provides a complete text summarization application with professional export capabilities.\n",
        "\n",
        "### ‚ú® Features:\n",
        "- ü§ñ Multiple AI models (BART, T5, Pegasus)\n",
        "- üìÑ **Markdown export** for documentation\n",
        "- üìä **JSON export** for APIs and data processing\n",
        "- üéµ **Audio export** (Text-to-Speech in 10 languages)\n",
        "- üìë **PDF export** for professional reports\n",
        "- ‚ö° Intelligent caching system\n",
        "- üìä Detailed statistics\n",
        "- üéØ Customizable summary length\n",
        "\n",
        "### üìã Requirements:\n",
        "- Python 3.8+\n",
        "- Works in Google Colab ‚úÖ\n",
        "- Works in VS Code ‚úÖ\n",
        "- Works in Jupyter Lab ‚úÖ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jut7FiFJEm3t"
      },
      "source": [
        "## Step 1: Install Required Dependencies\n",
        "\n",
        "Run this cell to install all necessary packages including export libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq7MMsa5Em3u",
        "outputId": "58459ef4-20e0-467f-b548-46df1cb8d204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing packages...\n",
            "‚úÖ All packages installed successfully!\n",
            "‚úÖ FFmpeg installed for Colab audio support\n"
          ]
        }
      ],
      "source": [
        "# Install all required packages\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Core packages\n",
        "packages = [\n",
        "    'gradio',\n",
        "    'transformers',\n",
        "    'torch',\n",
        "    'sentencepiece',\n",
        "    'protobuf',\n",
        "    'accelerate',\n",
        "    'gtts',  # Text-to-speech\n",
        "    'pydub',  # Audio processing\n",
        "    'markdown',  # Markdown processing\n",
        "    'reportlab'  # PDF generation\n",
        "]\n",
        "\n",
        "print(\"üì¶ Installing packages...\")\n",
        "for package in packages:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")\n",
        "\n",
        "# For Google Colab audio support (optional)\n",
        "try:\n",
        "    import google.colab\n",
        "    !apt-get install -y ffmpeg > /dev/null 2>&1\n",
        "    print(\"‚úÖ FFmpeg installed for Colab audio support\")\n",
        "except:\n",
        "    pass  # Not in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meE_s852Em3v"
      },
      "source": [
        "## Step 2: Import Libraries and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "lF17w7PrEm3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85b15953-69ea-4cc6-dd30-0fa5ae8c6cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Package Versions:\n",
            "  - Python: 3.12.12\n",
            "  - PyTorch: 2.8.0+cu126\n",
            "  - Transformers: 4.57.1\n",
            "  - Gradio: 5.49.1\n",
            "\n",
            "üñ•Ô∏è System:\n",
            "  - Device: CPU\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import logging\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "import json\n",
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning libraries\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    pipeline,\n",
        "    logging as transformers_logging\n",
        ")\n",
        "\n",
        "# Export libraries\n",
        "from gtts import gTTS\n",
        "import markdown\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib.units import inch\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.lib.enums import TA_JUSTIFY, TA_CENTER\n",
        "\n",
        "# Set transformers logging to error only\n",
        "transformers_logging.set_verbosity_error()\n",
        "\n",
        "# Print versions\n",
        "print(\"üì¶ Package Versions:\")\n",
        "print(f\"  - Python: {sys.version.split()[0]}\")\n",
        "print(f\"  - PyTorch: {torch.__version__}\")\n",
        "print(f\"  - Transformers: {__import__('transformers').__version__}\")\n",
        "print(f\"  - Gradio: {gr.__version__}\")\n",
        "print(f\"\\nüñ•Ô∏è System:\")\n",
        "print(f\"  - Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  - GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oZpIqjzEm3v"
      },
      "source": [
        "## Step 3: Configuration\n",
        "\n",
        "Central configuration for models, export settings, and system parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5GA64bcTEm3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570cae0d-a936-437b-ba5a-170e953b1c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Configuration loaded\n",
            "üìÅ Export directory: ./exports\n",
            "üìÅ Cache directory: ./model_cache\n",
            "üñ•Ô∏è Device: cpu\n",
            "ü§ñ Available models: 4\n"
          ]
        }
      ],
      "source": [
        "class Config:\n",
        "    \"\"\"\n",
        "    Central configuration class for the application.\n",
        "    Includes model settings and export configurations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Hugging Face Token (Optional)\n",
        "    HF_TOKEN = os.environ.get('HF_TOKEN', None)\n",
        "\n",
        "    # Export settings\n",
        "    EXPORT_DIR = \"./exports\"\n",
        "    AUDIO_LANGUAGES = {\n",
        "        \"English\": \"en\",\n",
        "        \"Spanish\": \"es\",\n",
        "        \"French\": \"fr\",\n",
        "        \"German\": \"de\",\n",
        "        \"Italian\": \"it\",\n",
        "        \"Portuguese\": \"pt\",\n",
        "        \"Hindi\": \"hi\",\n",
        "        \"Chinese\": \"zh\",\n",
        "        \"Japanese\": \"ja\",\n",
        "        \"Korean\": \"ko\"\n",
        "    }\n",
        "\n",
        "    # Model configurations\n",
        "    MODELS = {\n",
        "        \"facebook/bart-large-cnn\": {\n",
        "            \"name\": \"BART (CNN/DailyMail)\",\n",
        "            \"description\": \"Best for news articles\",\n",
        "            \"max_input_length\": 1024,\n",
        "            \"max_output_length\": 142,\n",
        "            \"min_output_length\": 56\n",
        "        },\n",
        "        \"t5-small\": {\n",
        "            \"name\": \"T5 Small (Fast)\",\n",
        "            \"description\": \"Lightweight and fast\",\n",
        "            \"max_input_length\": 512,\n",
        "            \"max_output_length\": 150,\n",
        "            \"min_output_length\": 40,\n",
        "            \"prefix\": \"summarize: \"\n",
        "        },\n",
        "        \"t5-base\": {\n",
        "            \"name\": \"T5 Base (Balanced)\",\n",
        "            \"description\": \"Good balance\",\n",
        "            \"max_input_length\": 512,\n",
        "            \"max_output_length\": 150,\n",
        "            \"min_output_length\": 40,\n",
        "            \"prefix\": \"summarize: \"\n",
        "        },\n",
        "        \"google/pegasus-xsum\": {\n",
        "            \"name\": \"Pegasus XSUM\",\n",
        "            \"description\": \"Abstractive summaries\",\n",
        "            \"max_input_length\": 512,\n",
        "            \"max_output_length\": 128,\n",
        "            \"min_output_length\": 30\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # System settings\n",
        "    CACHE_DIR = \"./model_cache\"\n",
        "    ENABLE_CACHE = True\n",
        "    MAX_CACHE_SIZE = 100\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    NUM_BEAMS = 4\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(Config.EXPORT_DIR, exist_ok=True)\n",
        "os.makedirs(Config.CACHE_DIR, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Configuration loaded\")\n",
        "print(f\"üìÅ Export directory: {Config.EXPORT_DIR}\")\n",
        "print(f\"üìÅ Cache directory: {Config.CACHE_DIR}\")\n",
        "print(f\"üñ•Ô∏è Device: {Config.DEVICE}\")\n",
        "print(f\"ü§ñ Available models: {len(Config.MODELS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNi1XRjNEm3w"
      },
      "source": [
        "## Step 4: Export Manager\n",
        "\n",
        "Handles exporting summaries to Markdown, JSON, Audio, and PDF formats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ltzxRmMvEm3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb9644e-162e-4f27-f2a6-07c00bb532d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Export Manager initialized\n",
            "üìù Supported formats: Markdown, JSON, Audio, PDF\n"
          ]
        }
      ],
      "source": [
        "class ExportManager:\n",
        "    \"\"\"\n",
        "    Manages export functionality for multiple formats.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.export_dir = Config.EXPORT_DIR\n",
        "        os.makedirs(self.export_dir, exist_ok=True)\n",
        "\n",
        "    def generate_filename(self, extension: str) -> str:\n",
        "        \"\"\"Generate unique filename with timestamp.\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        return f\"summary_{timestamp}.{extension}\"\n",
        "\n",
        "    def export_to_markdown(self, original_text: str, summary: str,\n",
        "                          statistics: Dict, model_name: str) -> str:\n",
        "        \"\"\"Export to Markdown format.\"\"\"\n",
        "        filename = self.generate_filename(\"md\")\n",
        "        filepath = os.path.join(self.export_dir, filename)\n",
        "\n",
        "        md_content = f\"\"\"# Text Summary Report\n",
        "\n",
        "## Metadata\n",
        "- **Date**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "- **Model**: {Config.MODELS[model_name]['name']}\n",
        "- **Compression**: {statistics['compression_ratio']}\n",
        "\n",
        "## Generated Summary\n",
        "{summary}\n",
        "\n",
        "## Statistics\n",
        "- Original: {statistics['original_words']} words\n",
        "- Summary: {statistics['summary_words']} words\n",
        "\n",
        "## Original Text\n",
        "<details>\n",
        "<summary>Click to expand</summary>\n",
        "\n",
        "{original_text}\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "*Generated using Text Summarization MVP*\n",
        "\"\"\"\n",
        "\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            f.write(md_content)\n",
        "\n",
        "        return filepath\n",
        "\n",
        "    def export_to_json(self, original_text: str, summary: str,\n",
        "                      statistics: Dict, model_name: str) -> str:\n",
        "        \"\"\"Export to JSON format.\"\"\"\n",
        "        filename = self.generate_filename(\"json\")\n",
        "        filepath = os.path.join(self.export_dir, filename)\n",
        "\n",
        "        json_data = {\n",
        "            \"metadata\": {\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"model\": {\n",
        "                    \"id\": model_name,\n",
        "                    \"name\": Config.MODELS[model_name]['name']\n",
        "                },\n",
        "                \"version\": \"1.0.0\"\n",
        "            },\n",
        "            \"content\": {\n",
        "                \"original_text\": original_text,\n",
        "                \"summary\": summary\n",
        "            },\n",
        "            \"statistics\": statistics\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        return filepath\n",
        "\n",
        "    def export_to_audio(self, text: str, language: str = \"en\",\n",
        "                       slow: bool = False) -> str:\n",
        "        \"\"\"Export to audio using text-to-speech.\"\"\"\n",
        "        try:\n",
        "            filename = self.generate_filename(\"mp3\")\n",
        "            filepath = os.path.join(self.export_dir, filename)\n",
        "\n",
        "            # Create audio\n",
        "            tts = gTTS(text=text, lang=language, slow=slow)\n",
        "            tts.save(filepath)\n",
        "\n",
        "            return filepath\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Audio export failed: {str(e)}\")\n",
        "\n",
        "    def export_to_pdf(self, original_text: str, summary: str,\n",
        "                     statistics: Dict, model_name: str) -> str:\n",
        "        \"\"\"Export to PDF format.\"\"\"\n",
        "        filename = self.generate_filename(\"pdf\")\n",
        "        filepath = os.path.join(self.export_dir, filename)\n",
        "\n",
        "        try:\n",
        "            doc = SimpleDocTemplate(filepath, pagesize=letter)\n",
        "            elements = []\n",
        "            styles = getSampleStyleSheet()\n",
        "\n",
        "            # Title\n",
        "            title_style = ParagraphStyle(\n",
        "                'CustomTitle',\n",
        "                parent=styles['Heading1'],\n",
        "                alignment=TA_CENTER,\n",
        "                spaceAfter=30\n",
        "            )\n",
        "            elements.append(Paragraph(\"Text Summary Report\", title_style))\n",
        "            elements.append(Spacer(1, 12))\n",
        "\n",
        "            # Metadata\n",
        "            metadata = f\"\"\"\n",
        "            <b>Date:</b> {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}<br/>\n",
        "            <b>Model:</b> {Config.MODELS[model_name]['name']}<br/>\n",
        "            <b>Compression:</b> {statistics['compression_ratio']}\n",
        "            \"\"\"\n",
        "            elements.append(Paragraph(metadata, styles['Normal']))\n",
        "            elements.append(Spacer(1, 20))\n",
        "\n",
        "            # Summary\n",
        "            elements.append(Paragraph(\"<b>Generated Summary</b>\", styles['Heading2']))\n",
        "            elements.append(Spacer(1, 12))\n",
        "            elements.append(Paragraph(summary, styles['BodyText']))\n",
        "\n",
        "            doc.build(elements)\n",
        "            return filepath\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"PDF export failed: {str(e)}\")\n",
        "\n",
        "# Initialize export manager\n",
        "export_manager = ExportManager()\n",
        "print(\"‚úÖ Export Manager initialized\")\n",
        "print(\"üìù Supported formats: Markdown, JSON, Audio, PDF\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wJRr_58Em3w"
      },
      "source": [
        "## Step 5: Cache Manager\n",
        "\n",
        "Handles caching of summaries for improved performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cMQk6taREm3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58183a85-a047-4d02-b338-7c64be9fe0fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Loaded 2 cached summaries\n",
            "‚úÖ Cache Manager initialized\n"
          ]
        }
      ],
      "source": [
        "class CacheManager:\n",
        "    \"\"\"Manages caching of summaries.\"\"\"\n",
        "\n",
        "    def __init__(self, cache_dir: str = Config.CACHE_DIR):\n",
        "        self.cache_dir = cache_dir\n",
        "        self.cache_file = os.path.join(cache_dir, \"summary_cache.json\")\n",
        "        self.cache = {}\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        self.load_cache()\n",
        "\n",
        "    def load_cache(self):\n",
        "        \"\"\"Load existing cache.\"\"\"\n",
        "        if os.path.exists(self.cache_file):\n",
        "            try:\n",
        "                with open(self.cache_file, 'r', encoding='utf-8') as f:\n",
        "                    self.cache = json.load(f)\n",
        "                print(f\"üìÇ Loaded {len(self.cache)} cached summaries\")\n",
        "            except:\n",
        "                self.cache = {}\n",
        "\n",
        "    def save_cache(self):\n",
        "        \"\"\"Save cache to file.\"\"\"\n",
        "        try:\n",
        "            with open(self.cache_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.cache, f, ensure_ascii=False, indent=2)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def get_cache_key(self, text: str, model_name: str, max_length: int) -> str:\n",
        "        \"\"\"Generate cache key.\"\"\"\n",
        "        content = f\"{text}_{model_name}_{max_length}\"\n",
        "        return hashlib.md5(content.encode()).hexdigest()\n",
        "\n",
        "    def get(self, text: str, model_name: str, max_length: int) -> Optional[str]:\n",
        "        \"\"\"Get cached summary.\"\"\"\n",
        "        if not Config.ENABLE_CACHE:\n",
        "            return None\n",
        "        key = self.get_cache_key(text, model_name, max_length)\n",
        "        return self.cache.get(key)\n",
        "\n",
        "    def set(self, text: str, model_name: str, max_length: int, summary: str):\n",
        "        \"\"\"Store summary in cache.\"\"\"\n",
        "        if not Config.ENABLE_CACHE:\n",
        "            return\n",
        "\n",
        "        if len(self.cache) >= Config.MAX_CACHE_SIZE:\n",
        "            oldest_key = next(iter(self.cache))\n",
        "            del self.cache[oldest_key]\n",
        "\n",
        "        key = self.get_cache_key(text, model_name, max_length)\n",
        "        self.cache[key] = summary\n",
        "        self.save_cache()\n",
        "\n",
        "cache_manager = CacheManager()\n",
        "print(\"‚úÖ Cache Manager initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u3ngGs2Em3x"
      },
      "source": [
        "## Step 6: Model Manager\n",
        "\n",
        "Handles loading and managing AI models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "-cm-xUaGEm3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7be52e5-e4f4-4ea1-e59f-302d5e1a202c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ ModelManager initialized on cpu\n",
            "‚úÖ Model Manager ready\n"
          ]
        }
      ],
      "source": [
        "class ModelManager:\n",
        "    \"\"\"Manages loading and caching of AI models.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loaded_models = {}\n",
        "        print(f\"ü§ñ ModelManager initialized on {Config.DEVICE}\")\n",
        "\n",
        "    def load_model(self, model_id: str) -> Tuple:\n",
        "        \"\"\"Load model and tokenizer.\"\"\"\n",
        "        if model_id in self.loaded_models:\n",
        "            print(f\"üìÇ Using cached model: {model_id}\")\n",
        "            return self.loaded_models[model_id]\n",
        "\n",
        "        try:\n",
        "            print(f\"‚è≥ Loading model: {model_id}...\")\n",
        "\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\n",
        "                model_id,\n",
        "                use_auth_token=Config.HF_TOKEN,\n",
        "                cache_dir=Config.CACHE_DIR\n",
        "            )\n",
        "\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "                model_id,\n",
        "                use_auth_token=Config.HF_TOKEN,\n",
        "                cache_dir=Config.CACHE_DIR,\n",
        "                device_map=\"auto\" if Config.DEVICE == \"cuda\" else None,\n",
        "                torch_dtype=torch.float16 if Config.DEVICE == \"cuda\" else torch.float32\n",
        "            )\n",
        "\n",
        "            if Config.DEVICE == \"cpu\":\n",
        "                model = model.to(Config.DEVICE)\n",
        "\n",
        "            self.loaded_models[model_id] = (tokenizer, model)\n",
        "            print(f\"‚úÖ Model loaded: {Config.MODELS[model_id]['name']}\")\n",
        "            return tokenizer, model\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load model: {str(e)}\")\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Clear loaded models.\"\"\"\n",
        "        self.loaded_models.clear()\n",
        "        if Config.DEVICE == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        print(\"üóëÔ∏è Model cache cleared\")\n",
        "\n",
        "model_manager = ModelManager()\n",
        "print(\"‚úÖ Model Manager ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gbcaOMnEm3x"
      },
      "source": [
        "## Step 7: Text Processing Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-CZZMZrnEm3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd1336f1-d0a9-464d-8f5d-b3a56ef4a3b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Text Processor ready\n"
          ]
        }
      ],
      "source": [
        "class TextProcessor:\n",
        "    \"\"\"Text preprocessing and postprocessing.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(text: str) -> str:\n",
        "        \"\"\"Clean input text.\"\"\"\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        if len(text) < 50:\n",
        "            raise ValueError(\"Text too short! Minimum 50 characters.\")\n",
        "\n",
        "        if len(text) > 50000:\n",
        "            text = text[:50000]\n",
        "\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def postprocess(summary: str) -> str:\n",
        "        \"\"\"Clean generated summary.\"\"\"\n",
        "        summary = ' '.join(summary.split())\n",
        "\n",
        "        if summary and summary[0].islower():\n",
        "            summary = summary[0].upper() + summary[1:]\n",
        "\n",
        "        if summary and summary[-1] not in '.!?':\n",
        "            summary += '.'\n",
        "\n",
        "        return summary\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_statistics(original: str, summary: str) -> Dict:\n",
        "        \"\"\"Calculate statistics.\"\"\"\n",
        "        original_words = len(original.split())\n",
        "        summary_words = len(summary.split())\n",
        "        compression = (1 - summary_words / original_words) * 100 if original_words > 0 else 0\n",
        "\n",
        "        return {\n",
        "            \"original_length\": len(original),\n",
        "            \"original_words\": original_words,\n",
        "            \"summary_length\": len(summary),\n",
        "            \"summary_words\": summary_words,\n",
        "            \"compression_ratio\": f\"{compression:.1f}%\"\n",
        "        }\n",
        "\n",
        "text_processor = TextProcessor()\n",
        "print(\"‚úÖ Text Processor ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBVcxR95Em3x"
      },
      "source": [
        "## Step 8: Summarization Engine\n",
        "\n",
        "Core engine that orchestrates the summarization process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "9J7HjeyoEm3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26c045e7-0477-483e-b1ef-ee5a9abfbeee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Summarization Engine ready\n"
          ]
        }
      ],
      "source": [
        "class SummarizationEngine:\n",
        "    \"\"\"Core summarization engine.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model_manager = model_manager\n",
        "        self.text_processor = text_processor\n",
        "        self.cache_manager = cache_manager\n",
        "        self.export_manager = export_manager\n",
        "        self.last_result = None\n",
        "\n",
        "    def summarize(self, text: str, model_name: str,\n",
        "                 max_length_ratio: float = 0.3) -> Tuple:\n",
        "        \"\"\"Generate summary.\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            if not text:\n",
        "                raise ValueError(\"Please provide text to summarize.\")\n",
        "\n",
        "            if model_name not in Config.MODELS:\n",
        "                raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "            # Preprocess\n",
        "            text = self.text_processor.preprocess(text)\n",
        "            model_config = Config.MODELS[model_name]\n",
        "\n",
        "            # Calculate max length\n",
        "            input_words = len(text.split())\n",
        "            max_length = min(\n",
        "                int(input_words * max_length_ratio),\n",
        "                model_config[\"max_output_length\"]\n",
        "            )\n",
        "            max_length = max(max_length, model_config[\"min_output_length\"])\n",
        "\n",
        "            # Check cache\n",
        "            cached_summary = self.cache_manager.get(text, model_name, max_length)\n",
        "            if cached_summary:\n",
        "                print(\"‚ö° Using cached summary\")\n",
        "                stats = self.text_processor.calculate_statistics(text, cached_summary)\n",
        "                processing_time = time.time() - start_time\n",
        "\n",
        "                self.last_result = {\n",
        "                    \"original_text\": text,\n",
        "                    \"summary\": cached_summary,\n",
        "                    \"statistics\": stats,\n",
        "                    \"model_name\": model_name\n",
        "                }\n",
        "                return cached_summary, stats, processing_time\n",
        "\n",
        "            # Load model\n",
        "            tokenizer, model = self.model_manager.load_model(model_name)\n",
        "\n",
        "            # Add prefix if needed\n",
        "            if \"prefix\" in model_config:\n",
        "                text_input = model_config[\"prefix\"] + text\n",
        "            else:\n",
        "                text_input = text\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = tokenizer(\n",
        "                text_input,\n",
        "                max_length=model_config[\"max_input_length\"],\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(Config.DEVICE)\n",
        "\n",
        "            print(f\"‚è≥ Generating summary...\")\n",
        "\n",
        "            # Generate\n",
        "            with torch.no_grad():\n",
        "                summary_ids = model.generate(\n",
        "                    inputs[\"input_ids\"],\n",
        "                    max_length=max_length,\n",
        "                    min_length=model_config[\"min_output_length\"],\n",
        "                    num_beams=Config.NUM_BEAMS,\n",
        "                    length_penalty=2.0,\n",
        "                    early_stopping=True,\n",
        "                    do_sample=False\n",
        "                )\n",
        "\n",
        "            # Decode\n",
        "            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "            summary = self.text_processor.postprocess(summary)\n",
        "\n",
        "            # Cache result\n",
        "            self.cache_manager.set(text, model_name, max_length, summary)\n",
        "\n",
        "            # Calculate statistics\n",
        "            stats = self.text_processor.calculate_statistics(text, summary)\n",
        "\n",
        "            processing_time = time.time() - start_time\n",
        "            print(f\"‚úÖ Summary generated in {processing_time:.2f} seconds\")\n",
        "\n",
        "            # Store for export\n",
        "            self.last_result = {\n",
        "                \"original_text\": text,\n",
        "                \"summary\": summary,\n",
        "                \"statistics\": stats,\n",
        "                \"model_name\": model_name\n",
        "            }\n",
        "\n",
        "            return summary, stats, processing_time\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Summarization failed: {str(e)}\")\n",
        "\n",
        "# Initialize engine\n",
        "engine = SummarizationEngine()\n",
        "print(\"‚úÖ Summarization Engine ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0hYK1XzEm3x"
      },
      "source": [
        "## Step 9: Test the Engine\n",
        "\n",
        "Let's test the summarization with a sample text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hPlMMNJ1Em3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286c65fa-1d35-4765-9e36-613004ef6e11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° Using cached summary\n",
            "\n",
            "============================================================\n",
            "üìÑ SUMMARY:\n",
            "Artificial intelligence has become one of the most transformative technologies of our time. from virtual assistants to self-driving cars, AI is reshaping how we live and.\n",
            "\n",
            "üìä STATISTICS:\n",
            "  original_length: 510\n",
            "  original_words: 67\n",
            "  summary_length: 170\n",
            "  summary_words: 26\n",
            "  compression_ratio: 61.2%\n",
            "\n",
            "‚è±Ô∏è Time: 0.00 seconds\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Test with sample text\n",
        "sample_text = \"\"\"\n",
        "Artificial Intelligence has become one of the most transformative technologies\n",
        "of our time. From virtual assistants to self-driving cars, AI is reshaping\n",
        "how we live and work. Machine learning algorithms power recommendation systems,\n",
        "detect fraud, and assist in medical diagnostics. Natural language processing\n",
        "enables human-like text generation and understanding. As AI continues to evolve,\n",
        "ethical considerations around bias, privacy, and employment become increasingly\n",
        "important for responsible development.\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    # Generate summary using T5-small\n",
        "    summary, stats, time_taken = engine.summarize(\n",
        "        text=sample_text,\n",
        "        model_name=\"t5-small\",\n",
        "        max_length_ratio=0.3\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìÑ SUMMARY:\")\n",
        "    print(summary)\n",
        "    print(\"\\nüìä STATISTICS:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print(f\"\\n‚è±Ô∏è Time: {time_taken:.2f} seconds\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkN9c6lyEm3y"
      },
      "source": [
        "## Step 10: Export Functions\n",
        "\n",
        "Test the export functionality with different formats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "bYqeDrLLEm3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c503996-362e-4742-f772-360225c9a466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì¶ Testing export functions:\n",
            "‚úÖ Exported to: ./exports/summary_20251028_185802.md\n",
            "‚úÖ Exported to: ./exports/summary_20251028_185802.json\n",
            "‚úÖ Exported to: ./exports/summary_20251028_185802.pdf\n",
            "‚úÖ Exported to: ./exports/summary_20251028_185802.mp3\n"
          ]
        }
      ],
      "source": [
        "# Export functions for the interface\n",
        "def export_markdown():\n",
        "    \"\"\"Export to Markdown.\"\"\"\n",
        "    if not engine.last_result:\n",
        "        return None, \"‚ùå No summary to export. Generate a summary first.\"\n",
        "\n",
        "    try:\n",
        "        filepath = engine.export_manager.export_to_markdown(\n",
        "            **engine.last_result\n",
        "        )\n",
        "        # Return filepath instead of auto-downloading\n",
        "        return filepath, f\"‚úÖ Exported to: {filepath}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"‚ùå Export failed: {str(e)}\"\n",
        "\n",
        "\n",
        "def export_json():\n",
        "    \"\"\"Export to JSON.\"\"\"\n",
        "    if not engine.last_result:\n",
        "        return None, \"‚ùå No summary to export. Generate a summary first.\"\n",
        "\n",
        "    try:\n",
        "        filepath = engine.export_manager.export_to_json(\n",
        "            **engine.last_result\n",
        "        )\n",
        "        # Return filepath instead of auto-downloading\n",
        "        return filepath, f\"‚úÖ Exported to: {filepath}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"‚ùå Export failed: {str(e)}\"\n",
        "\n",
        "def export_audio(language: str, slow: bool):\n",
        "    \"\"\"Export to audio.\"\"\"\n",
        "    if not engine.last_result:\n",
        "        # Return None for audio_output and audio_file, and an error message for audio_status\n",
        "        return None, None, \"‚ùå No summary to export.\"\n",
        "\n",
        "    try:\n",
        "        lang_code = Config.AUDIO_LANGUAGES.get(language, \"en\")\n",
        "        filepath = engine.export_manager.export_to_audio(\n",
        "            text=engine.last_result[\"summary\"],\n",
        "            language=lang_code,\n",
        "            slow=slow\n",
        "        )\n",
        "        # Return the filepath for both the audio playback and download link, and a success message\n",
        "        return filepath, filepath, f\"‚úÖ Exported to: {filepath}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return None for audio_output and audio_file, and an error message for audio_status\n",
        "        return None, None, f\"‚ùå Export failed: {str(e)}\"\n",
        "\n",
        "\n",
        "def export_pdf():\n",
        "    \"\"\"Export to PDF.\"\"\"\n",
        "    if not engine.last_result:\n",
        "        return None, \"‚ùå No summary to export. Generate a summary first.\"\n",
        "\n",
        "    try:\n",
        "        filepath = engine.export_manager.export_to_pdf(\n",
        "            **engine.last_result\n",
        "        )\n",
        "        # Return filepath instead of auto-downloading\n",
        "        return filepath, f\"‚úÖ Exported to: {filepath}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"‚ùå Export failed: {str(e)}\"\n",
        "\n",
        "\n",
        "# Test exports (if summary was generated)\n",
        "if engine.last_result:\n",
        "    print(\"\\nüì¶ Testing export functions:\")\n",
        "    # Update test calls to match new return values\n",
        "    md_file, md_status = export_markdown()\n",
        "    print(md_status)\n",
        "    json_file, json_status = export_json()\n",
        "    print(json_status)\n",
        "    pdf_file, pdf_status = export_pdf()\n",
        "    print(pdf_status)\n",
        "    audio_output_test, audio_file_test, audio_status_test = export_audio(\"English\", False)\n",
        "    print(audio_status_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOPJc96iEm3y"
      },
      "source": [
        "## Step 11: Create Gradio Interface with Export Features\n",
        "\n",
        "Build the complete web interface with export functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "h6LAxWpaEm3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c78b6a5-0a1b-4aaa-b8c3-db3c9bd81002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Interface functions ready\n"
          ]
        }
      ],
      "source": [
        "def process_summary(text: str, model_name: str, summary_length: float) -> Tuple:\n",
        "    \"\"\"Process summarization request.\"\"\"\n",
        "    try:\n",
        "        summary, stats, time_taken = engine.summarize(\n",
        "            text=text,\n",
        "            model_name=model_name,\n",
        "            max_length_ratio=summary_length\n",
        "        )\n",
        "\n",
        "        stats_html = f\"\"\"\n",
        "        <div style='background-color: #f0f0f0; padding: 10px; border-radius: 5px;'>\n",
        "            <h4>üìä Summary Statistics</h4>\n",
        "            <ul>\n",
        "                <li><strong>Original:</strong> {stats['original_words']} words</li>\n",
        "                <li><strong>Summary:</strong> {stats['summary_words']} words</li>\n",
        "                <li><strong>Compression:</strong> {stats['compression_ratio']}</li>\n",
        "                <li><strong>Time:</strong> {time_taken:.2f} seconds</li>\n",
        "                <li><strong>Model:</strong> {Config.MODELS[model_name]['name']}</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        status = \"‚úÖ Summary generated! You can now export it.\"\n",
        "        return summary, stats_html, status\n",
        "\n",
        "    except Exception as e:\n",
        "        error_html = f\"<div style='color: red;'>‚ùå Error: {str(e)}</div>\"\n",
        "        return \"\", error_html, f\"‚ùå Error: {str(e)}\"\n",
        "\n",
        "print(\"‚úÖ Interface functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ZmicpgWGEm3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47b28901-659f-45f6-b312-d0dccba968dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gradio interface created with export features\n"
          ]
        }
      ],
      "source": [
        "# Create the Gradio interface with export features\n",
        "with gr.Blocks(title=\"Text Summarization with Export\", theme=gr.themes.Soft()) as interface:\n",
        "\n",
        "    # Header\n",
        "    gr.Markdown(\"# üöÄ Text Summarization with Export Features\")\n",
        "    gr.Markdown(\"\"\"\n",
        "    Generate AI-powered summaries and export them in multiple formats:\n",
        "    Markdown | JSON | Audio | PDF\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # Summarize Tab\n",
        "        with gr.TabItem(\"üìù Summarize\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    model_dropdown = gr.Dropdown(\n",
        "                        choices=list(Config.MODELS.keys()),\n",
        "                        value=\"t5-small\",\n",
        "                        label=\"ü§ñ Select Model\"\n",
        "                    )\n",
        "\n",
        "                    length_slider = gr.Slider(\n",
        "                        minimum=0.1,\n",
        "                        maximum=0.5,\n",
        "                        value=0.3,\n",
        "                        step=0.05,\n",
        "                        label=\"üìè Summary Length\"\n",
        "                    )\n",
        "\n",
        "                    text_input = gr.Textbox(\n",
        "                        label=\"üìù Enter Text\",\n",
        "                        placeholder=\"Paste your text here...\",\n",
        "                        lines=15\n",
        "                    )\n",
        "\n",
        "                    submit_btn = gr.Button(\"üöÄ Generate Summary\", variant=\"primary\")\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    status_output = gr.Textbox(\n",
        "                        label=\"Status\",\n",
        "                        value=\"Ready\",\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "                    summary_output = gr.Textbox(\n",
        "                        label=\"üìÑ Summary\",\n",
        "                        lines=10,\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "                    stats_output = gr.HTML()\n",
        "\n",
        "                    clear_btn = gr.Button(\"üóëÔ∏è Clear\", variant=\"secondary\")\n",
        "\n",
        "        # Export Tab\n",
        "        with gr.TabItem(\"üíæ Export\"):\n",
        "            gr.Markdown(\"### Export Your Summary\")\n",
        "            gr.Markdown(\"Generate a summary first, then export:\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"#### üìÑ Markdown\")\n",
        "                    md_btn = gr.Button(\"Export as Markdown\", variant=\"primary\")\n",
        "                    # Use gr.File to display downloadable link\n",
        "                    md_file = gr.File(label=\"Download Markdown\")\n",
        "                    md_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"#### üìä JSON\")\n",
        "                    json_btn = gr.Button(\"Export as JSON\", variant=\"primary\")\n",
        "                     # Use gr.File to display downloadable link\n",
        "                    json_file = gr.File(label=\"Download JSON\")\n",
        "                    json_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"#### üéµ Audio\")\n",
        "                    audio_lang = gr.Dropdown(\n",
        "                        choices=list(Config.AUDIO_LANGUAGES.keys()),\n",
        "                        value=\"English\",\n",
        "                        label=\"Language\"\n",
        "                    )\n",
        "                    audio_slow = gr.Checkbox(label=\"Slow Speech\", value=False)\n",
        "                    audio_btn = gr.Button(\"Export as Audio\", variant=\"primary\")\n",
        "                    # Use gr.Audio with type=\"filepath\" to display and play audio\n",
        "                    audio_output = gr.Audio(label=\"Audio Playback\", type=\"filepath\")\n",
        "                     # Use gr.File to display downloadable link for audio\n",
        "                    audio_file = gr.File(label=\"Download Audio\")\n",
        "                    audio_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"#### üìë PDF\")\n",
        "                    pdf_btn = gr.Button(\"Export as PDF\", variant=\"primary\")\n",
        "                     # Use gr.File to display downloadable link\n",
        "                    pdf_file = gr.File(label=\"Download PDF\")\n",
        "                    pdf_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "\n",
        "        # Help Tab\n",
        "        with gr.TabItem(\"‚ùì Help\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### How to Use\n",
        "\n",
        "            1. **Summarize**: Select model, adjust length, paste text, generate\n",
        "            2. **Export**: Go to Export tab, choose format\n",
        "\n",
        "            ### Export Formats\n",
        "            - **Markdown**: Documentation, GitHub, blogs\n",
        "            - **JSON**: APIs, data processing\n",
        "            - **Audio**: Accessibility, listening\n",
        "            - **PDF**: Professional reports\n",
        "\n",
        "            Files are saved in `./exports/` directory.\n",
        "            \"\"\")\n",
        "\n",
        "    # Event handlers\n",
        "    submit_btn.click(\n",
        "        fn=process_summary,\n",
        "        inputs=[text_input, model_dropdown, length_slider],\n",
        "        outputs=[summary_output, stats_output, status_output]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=lambda: (\"\", \"\", \"\", \"Ready\"),\n",
        "        outputs=[text_input, summary_output, stats_output, status_output]\n",
        "    )\n",
        "\n",
        "    # Export handlers\n",
        "    md_btn.click(fn=export_markdown, outputs=[md_file, md_status])\n",
        "    json_btn.click(fn=export_json, outputs=[json_file, json_status])\n",
        "    audio_btn.click(\n",
        "        fn=export_audio,\n",
        "        inputs=[audio_lang, audio_slow],\n",
        "        outputs=[audio_output, audio_file, audio_status]\n",
        "    )\n",
        "    pdf_btn.click(fn=export_pdf, outputs=[pdf_file, pdf_status])\n",
        "\n",
        "\n",
        "print(\"‚úÖ Gradio interface created with export features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F21MRa6hEm3y"
      },
      "source": [
        "## Step 12: Launch the Application\n",
        "\n",
        "Start the web interface. It will create a public link for sharing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "TDd12vNtEm3y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "outputId": "643732b5-c5b6-4029-87bb-fb5b7d0d0742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Launching Text Summarization with Export Features...\n",
            "============================================================\n",
            "üìÅ Exports will be saved to: ./exports/\n",
            "üåê Creating web interface...\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://271cb1f8c3faec92c7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://271cb1f8c3faec92c7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "# Launch the application\n",
        "print(\"üöÄ Launching Text Summarization with Export Features...\")\n",
        "print(\"=\"*60)\n",
        "print(\"üìÅ Exports will be saved to: ./exports/\")\n",
        "print(\"üåê Creating web interface...\\n\")\n",
        "\n",
        "# Launch with sharing enabled\n",
        "interface.launch(\n",
        "    share=True,  # Creates public link\n",
        "    server_name=\"0.0.0.0\",\n",
        "    # server_port=7860, # Remove specific port to let Gradio find one\n",
        "    show_error=True,\n",
        "    quiet=False\n",
        ")\n",
        "\n",
        "# Note: The app will run until you stop this cell\n",
        "# In Google Colab, the public URL will be displayed\n",
        "# In VS Code/Jupyter, open http://localhost:7860"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Step 13: Download Exported Files (Google Colab)\n",
        "# ============================================\n",
        "# Run this cell AFTER you've generated and exported summaries\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üì• EXPORT FILE MANAGER FOR GOOGLE COLAB\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "    print(\"‚úÖ Google Colab detected\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"‚ÑπÔ∏è Not running in Colab\")\n",
        "\n",
        "def list_exports():\n",
        "    \"\"\"List all exported files\"\"\"\n",
        "    export_dir = '/content/exports' if IN_COLAB else './exports'\n",
        "\n",
        "    if os.path.exists(export_dir):\n",
        "        files_list = glob.glob(f'{export_dir}/*')\n",
        "        if files_list:\n",
        "            print(f\"\\nüìÅ Found {len(files_list)} exported files:\")\n",
        "            for file in sorted(files_list):\n",
        "                size = os.path.getsize(file) / 1024\n",
        "                name = os.path.basename(file)\n",
        "                print(f\"  ‚úÖ {name} ({size:.1f} KB)\")\n",
        "            return files_list\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Exports folder exists but is empty\")\n",
        "    else:\n",
        "        print(f\"‚ùå No exports folder found at {export_dir}\")\n",
        "    return []\n",
        "\n",
        "def download_all_exports():\n",
        "    \"\"\"Download all exported files to your computer\"\"\"\n",
        "    if not IN_COLAB:\n",
        "        print(\"‚ÑπÔ∏è Not in Colab - files are already on your computer\")\n",
        "        return\n",
        "\n",
        "    from google.colab import files\n",
        "    export_files = list_exports()\n",
        "\n",
        "    if export_files:\n",
        "        print(\"\\nüì• Downloading files to your computer...\")\n",
        "        for filepath in export_files:\n",
        "            try:\n",
        "                files.download(filepath)\n",
        "                print(f\"  ‚¨áÔ∏è Downloaded: {os.path.basename(filepath)}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå Failed: {os.path.basename(filepath)} - {e}\")\n",
        "        print(\"\\n‚úÖ Check your browser's Downloads folder!\")\n",
        "    else:\n",
        "        print(\"‚ùå No files to download\")\n",
        "\n",
        "def save_to_google_drive():\n",
        "    \"\"\"Save exports to Google Drive for permanent storage\"\"\"\n",
        "    if not IN_COLAB:\n",
        "        print(\"‚ÑπÔ∏è This feature is only for Google Colab\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        import shutil\n",
        "\n",
        "        # Mount Drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        # Create destination folder\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        dest = f'/content/drive/MyDrive/text_summaries_{timestamp}'\n",
        "        os.makedirs(dest, exist_ok=True)\n",
        "\n",
        "        # Copy files\n",
        "        export_files = list_exports()\n",
        "        if export_files:\n",
        "            print(f\"\\nüìÅ Copying to Google Drive: {dest}\")\n",
        "            for filepath in export_files:\n",
        "                shutil.copy2(filepath, dest)\n",
        "                print(f\"  ‚úÖ Copied: {os.path.basename(filepath)}\")\n",
        "            print(f\"\\n‚úÖ Files saved to Google Drive!\")\n",
        "            print(f\"üìç Location: /MyDrive/text_summaries_{timestamp}/\")\n",
        "        else:\n",
        "            print(\"‚ùå No files to save\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "def download_specific_file(filename):\n",
        "    \"\"\"Download a specific file\"\"\"\n",
        "    if not IN_COLAB:\n",
        "        print(\"‚ÑπÔ∏è Not in Colab - files are already on your computer\")\n",
        "        return\n",
        "\n",
        "    from google.colab import files\n",
        "    filepath = f'/content/exports/{filename}'\n",
        "\n",
        "    if os.path.exists(filepath):\n",
        "        files.download(filepath)\n",
        "        print(f\"‚úÖ Downloaded: {filename}\")\n",
        "    else:\n",
        "        print(f\"‚ùå File not found: {filename}\")\n",
        "\n",
        "# ============================================\n",
        "# AUTO-RUN OPTIONS\n",
        "# ============================================\n",
        "\n",
        "# 1. List all exported files\n",
        "list_exports()\n",
        "\n",
        "# 2. Create download buttons\n",
        "if IN_COLAB:\n",
        "    print(\"\\nüéØ QUICK ACTIONS:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"Run these commands to manage your files:\\n\")\n",
        "    print(\"1Ô∏è‚É£ Download ALL exports to computer:\")\n",
        "    print(\"   download_all_exports()\")\n",
        "    print(\"\\n2Ô∏è‚É£ Save to Google Drive (permanent):\")\n",
        "    print(\"   save_to_google_drive()\")\n",
        "    print(\"\\n3Ô∏è‚É£ Download specific file:\")\n",
        "    print(\"   download_specific_file('summary_20251028_180000.md')\")\n",
        "    print(\"\\n4Ô∏è‚É£ List files again:\")\n",
        "    print(\"   list_exports()\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Optional: Auto-download all files\n",
        "    # Uncomment the line below to auto-download when you run this cell\n",
        "    # download_all_exports()"
      ],
      "metadata": {
        "id": "tF4Z1gRpH9aF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2953ab7e-215e-4946-caba-5b9ccb09e655"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üì• EXPORT FILE MANAGER FOR GOOGLE COLAB\n",
            "============================================================\n",
            "‚úÖ Google Colab detected\n",
            "\n",
            "üìÅ Found 25 exported files:\n",
            "  ‚úÖ summary_20251028_182925.json (1.1 KB)\n",
            "  ‚úÖ summary_20251028_182925.md (1.0 KB)\n",
            "  ‚úÖ summary_20251028_182925.mp3 (98.1 KB)\n",
            "  ‚úÖ summary_20251028_182925.pdf (1.9 KB)\n",
            "  ‚úÖ summary_20251028_183221.md (1.3 KB)\n",
            "  ‚úÖ summary_20251028_183237.mp3 (122.8 KB)\n",
            "  ‚úÖ summary_20251028_183259.json (1.3 KB)\n",
            "  ‚úÖ summary_20251028_183301.pdf (1.9 KB)\n",
            "  ‚úÖ summary_20251028_183723.md (1.3 KB)\n",
            "  ‚úÖ summary_20251028_184323.json (1.1 KB)\n",
            "  ‚úÖ summary_20251028_184323.md (1.0 KB)\n",
            "  ‚úÖ summary_20251028_184323.mp3 (98.1 KB)\n",
            "  ‚úÖ summary_20251028_184323.pdf (1.9 KB)\n",
            "  ‚úÖ summary_20251028_184534.md (1.3 KB)\n",
            "  ‚úÖ summary_20251028_184545.mp3 (122.8 KB)\n",
            "  ‚úÖ summary_20251028_184549.json (1.3 KB)\n",
            "  ‚úÖ summary_20251028_184551.pdf (1.9 KB)\n",
            "  ‚úÖ summary_20251028_184939.json (1.1 KB)\n",
            "  ‚úÖ summary_20251028_184939.md (1.0 KB)\n",
            "  ‚úÖ summary_20251028_184940.mp3 (98.1 KB)\n",
            "  ‚úÖ summary_20251028_184940.pdf (1.9 KB)\n",
            "  ‚úÖ summary_20251028_185124.md (1.3 KB)\n",
            "  ‚úÖ summary_20251028_185147.mp3 (122.8 KB)\n",
            "  ‚úÖ summary_20251028_185152.json (1.3 KB)\n",
            "  ‚úÖ summary_20251028_185153.pdf (1.9 KB)\n",
            "\n",
            "üéØ QUICK ACTIONS:\n",
            "----------------------------------------\n",
            "Run these commands to manage your files:\n",
            "\n",
            "1Ô∏è‚É£ Download ALL exports to computer:\n",
            "   download_all_exports()\n",
            "\n",
            "2Ô∏è‚É£ Save to Google Drive (permanent):\n",
            "   save_to_google_drive()\n",
            "\n",
            "3Ô∏è‚É£ Download specific file:\n",
            "   download_specific_file('summary_20251028_180000.md')\n",
            "\n",
            "4Ô∏è‚É£ List files again:\n",
            "   list_exports()\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SS0Wj8tEm3y"
      },
      "source": [
        "## üìä Export Format Examples\n",
        "\n",
        "### Markdown Export Preview:\n",
        "```markdown\n",
        "# Text Summary Report\n",
        "- Date: 2025-10-28 18:00:00\n",
        "- Model: T5 Small\n",
        "- Compression: 70%\n",
        "\n",
        "## Summary\n",
        "AI is transforming technology through virtual assistants...\n",
        "```\n",
        "\n",
        "### JSON Export Preview:\n",
        "```json\n",
        "{\n",
        "  \"metadata\": {\n",
        "    \"timestamp\": \"2025-10-28T18:00:00\",\n",
        "    \"model\": \"t5-small\"\n",
        "  },\n",
        "  \"content\": {\n",
        "    \"summary\": \"...\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Audio Languages Supported:\n",
        "- üá∫üá∏ English\n",
        "- üá™üá∏ Spanish  \n",
        "- üá´üá∑ French\n",
        "- üá©üá™ German\n",
        "- üáÆüáπ Italian\n",
        "- üáµüáπ Portuguese\n",
        "- üáÆüá≥ Hindi\n",
        "- üá®üá≥ Chinese\n",
        "- üáØüáµ Japanese\n",
        "- üá∞üá∑ Korean\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ú® Tips\n",
        "\n",
        "1. **For Google Colab**: The public URL will work for 72 hours\n",
        "2. **For VS Code**: Open http://localhost:7860 in your browser\n",
        "3. **First run**: Models download automatically (one-time)\n",
        "4. **Export files**: Check the `./exports/` directory\n",
        "5. **Audio files**: Require internet for text-to-speech\n",
        "\n",
        "## üéØ Ready!\n",
        "\n",
        "Your enhanced text summarization tool with export features is ready to use!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}